# -*- coding: utf-8 -*-
"""AlexNet_Keras

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pOEsUWv0fIZmnxdEiXP40r6wyl7wi4Gj

https://ithelp.ithome.com.tw/m/articles/10240550
"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.utils import to_categorical
from tensorflow.keras import models
from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D,BatchNormalization, Dropout
from tensorflow.keras.datasets import cifar10
import random
import os

#2025/0207新增
seed_value = 1234
np.random.seed(seed_value)
random.seed(seed_value)
tf.random.set_seed(seed_value)
os.environ['PYTHONHASHSEED'] = str(seed_value)

(x_Train, y_Train), (x_Test, y_Test) = cifar10.load_data()

import matplotlib.pyplot as plt
# CIFAR-10 類別名稱
class_names = ["airplane", "automobile", "bird", "cat", "deer",
               "dog", "frog", "horse", "ship", "truck"]
# 顯示前 10 張圖片
plt.figure(figsize=(10,5))
for i in range(10):
    plt.subplot(2, 5, i+1)
    plt.imshow(x_Train[i])  # CIFAR-10 是 RGB 圖片
    plt.title(class_names[y_Train[i].argmax()])  # 轉換 One-hot 為類別
    plt.axis("off")

plt.show()

#做正規化，將原本範圍在 0~255 的像素值轉換到範圍 0~1 之間
# x_Train, x_Test, = x_Train / 255.0, x_Test / 255.0
y_Train = to_categorical(y_Train, 10)  # One-hot 編碼
y_Test = to_categorical(y_Test, 10)    # One-hot 編碼
#標準化standardization
mean = np.mean(x_Train, axis=(0,1,2))  # 對 height, width, channel 軸取均值
std = np.std(x_Train, axis=(0,1,2))    # 對 height, width, channel 軸取標準差
# print(f"Mean: {mean}, Std: {std}")
x_Train = (x_Train - mean) / std
x_Test = (x_Test - mean) / std

# 定義預處理函數
def preprocess(image, label):
    image = tf.image.resize(image, (227, 227))  # 只在訓練時調整大小
    # image = tf.cast(image, tf.float32) / 255.0  # 標準化
    return image, label
# 使用 `tf.data.Dataset` 來動態處理
train_ds = tf.data.Dataset.from_tensor_slices((x_Train, y_Train))
train_ds = train_ds.map(preprocess).batch(32).prefetch(tf.data.AUTOTUNE)
test_ds = tf.data.Dataset.from_tensor_slices((x_Test, y_Test))
test_ds = test_ds.map(preprocess).batch(32).prefetch(tf.data.AUTOTUNE)

#RAM爆了
# x_Train = tf.image.resize(x_Train, (227, 227))
# x_Test = tf.image.resize(x_Test, (227, 227))

model = models.Sequential()

#第一層
model.add(Conv2D(filters=96, kernel_size=(11, 11), strides=(4, 4), activation="relu", input_shape=(227, 227, 3)))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))
#第二層
model.add(Conv2D(filters=256, kernel_size=(5, 5), strides=(1, 1), activation="relu"))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))
#第三層
model.add(Conv2D(filters=384, kernel_size=(3, 3), strides=(1, 1), activation="relu"))
model.add(BatchNormalization())
#第四層
model.add(Conv2D(filters=384, kernel_size=(3, 3), strides=(1, 1), activation="relu"))
model.add(BatchNormalization())
#第五層
model.add(Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), activation="relu"))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))
model.add(Flatten())
model.add(Dense(4096, activation="relu"))
model.add(Dropout(0.5))
model.add(Dense(4096, activation="relu"))
model.add(Dropout(0.5))
model.add(Dense(10,activation="softmax"))
print(model.summary())

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

model.fit(train_ds, epochs=5)

model.evaluate(train_ds, verbose=2)

model.evaluate(test_ds, verbose=2)
